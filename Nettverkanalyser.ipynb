{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 1]\n",
      " [3 3]\n",
      " [2 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.               , 3.605551275463989, 4.47213595499958 ],\n",
       "       [3.605551275463989, 0.               , 1.               ],\n",
       "       [4.47213595499958 , 1.               , 0.               ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "rng = np.random.default_rng()\n",
    "X = rng.integers(low = 0, high = 10, size=(3,2))\n",
    "print(X)\n",
    "cdist(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method Generator.integers() needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(k,n)\n\u001b[1;32m---> 10\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscipy_cdist\u001b[39m(X,Y):\n\u001b[0;32m     13\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m cdist(X,Y,metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unbound method Generator.integers() needs an argument"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "m = 100 \n",
    "k = 200\n",
    "\n",
    "X = np.random.randn(m,2)\n",
    "\n",
    "Y = np.random.randn(k,n)\n",
    "\n",
    "\n",
    "X = np.random.Generator.integers(low = 0, high = 100, size = (50,2))\n",
    "\n",
    "def scipy_cdist(X,Y):\n",
    "  return cdist(X,Y,metric='euclidean')\n",
    "\n",
    "def numpy_for_loop(X,Y):\n",
    "    m = X.shape[0]\n",
    "    k = Y.shape[0]\n",
    "    D = np.zeros((m,k))\n",
    "    for i in range(m):\n",
    "        for j in range(k):\n",
    "            D[i,j] = np.sum((X[i] - Y[j])**2)**0.5\n",
    "    return D\n",
    "\n",
    "def numpy_vectorized(X,Y):\n",
    "    return np.sum((X[:,None,:] - Y[None,:,:])**2, axis=2)**0.5\n",
    "print(X)\n",
    "scipy_cdist(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arcpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39mset_printoptions(precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01marcpy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'arcpy'"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=20)\n",
    "import arcpy\n",
    "import os\n",
    "import scipy as scipy\n",
    "from scipy.linalg import eig\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def create_or_get_gdb(out_folder_path, gdb_name):\n",
    "    # Check if the geodatabase already exists\n",
    "    gdb_path = os.path.join(out_folder_path, gdb_name)\n",
    "    if arcpy.Exists(gdb_path):\n",
    "        print(f\"Geodatabase '{gdb_name}' already exists at {gdb_path}\")\n",
    "        return gdb_path\n",
    "    else:\n",
    "        # Create the geodatabase\n",
    "        arcpy.CreateFileGDB_management(out_folder_path, gdb_name)\n",
    "        print(f\"Geodatabase '{gdb_name}' created at {gdb_path}\")\n",
    "        return gdb_path    \n",
    "# Function to calculate distances, both Euclidian and \"realised\"/\"Experienced\" distances, where the latter is a probability output\n",
    "def calculate_distance_matrix(input_feature_class, Alpha = 1/1000, ExportLines = False):\n",
    "    \"\"\"\n",
    "    This function calculates a distance matrix for all points in the input feature class using NumPy arrays.\n",
    "    \n",
    "    :param input_feature_class: Path to the input feature class containing point geometries.\n",
    "    :return: A NumPy array representing the distance matrix.\n",
    "    \"\"\"\n",
    "    # Retrieve the spatial reference of the input feature class.\n",
    "    sr = arcpy.Describe(input_feature_class).spatialReference\n",
    "\n",
    "    # Get the number of points in the input feature class.\n",
    "    point_count = int(arcpy.GetCount_management(input_feature_class).getOutput(0))\n",
    "\n",
    "    # Initialize a NumPy array to store the distance matrix with zeros.\n",
    "    distance_matrix = np.zeros((point_count, point_count))\n",
    "    # Initialize a NumPy array to store the dispersal probabilities.\n",
    "    distance_matrix_exp = np.zeros((point_count, point_count))\n",
    "\n",
    "    # Create a list to store geometries and their IDs.\n",
    "    geometries = []    \n",
    "    \n",
    "    # Here we have chunk where we create a feature class where we can store the lines going btw all the features\n",
    "    # Define the spatial reference\n",
    "    spatial_ref = arcpy.Describe(input_feature_class).spatialReference\n",
    "    \n",
    "    # Use a search cursor to iterate through the features and store geometries.\n",
    "    with arcpy.da.SearchCursor(input_feature_class, [\"OID@\", \"SHAPE@\"], spatial_reference=sr) as cursor:\n",
    "        for row in cursor:\n",
    "            oid, geom = row\n",
    "            geometries.append((oid, geom))\n",
    "\n",
    "    # Calculate distances and fill the NumPy array.\n",
    "    for i, (oid_from, geom_from) in enumerate(geometries):\n",
    "        #print(i)\n",
    "        for j, (oid_to, geom_to) in enumerate(geometries):\n",
    "            if oid_from != oid_to:  # Ensure that we don't calculate the distance to the same feature.\n",
    "                distance = geom_from.distanceTo(geom_to)  # Calculate the distance to another feature.\n",
    "                distance_matrix[i, j] = distance  # Store the calculated distance in the array.\n",
    "                distance_matrix_exp[i, j] = np.exp(-distance*Alpha)  # Store the calculated distance in the array.\n",
    "                #if ExportLines is True\n",
    "                    # Create a new feature class\n",
    "                    #new_feature_class = 'Paths'\n",
    "                    #arcpy.CreateFeatureclass_management(arcpy.env.workspace, new_feature_class, 'POLYLINE', spatial_reference=spatial_ref)\n",
    "                    #start_point = geom_from.centroid # now we move over to generating lines btw all the geometries\n",
    "                #    end_point = geom_to.centroid  #  this lines are later used to illustrate sensitities of the connections\n",
    "                #    line = arcpy.Polyline(arcpy.Array([start_point, end_point]), spatial_ref) # create a line\n",
    "                #    # TO-DO: Add OID\n",
    "                #    # Use an insert cursor to add the new line to the feature class\n",
    "                #    with arcpy.da.InsertCursor(new_feature_class, ['SHAPE@']) as cursor:\n",
    "                #        cursor.insertRow([line])\n",
    "\n",
    "    return distance_matrix, distance_matrix_exp  # Return the complete distance matrix.\n",
    "\n",
    "def calculate_areas_and_outer_matrix(feature_class):\n",
    "    \"\"\"\n",
    "    Calculate the areas of features in a feature class and the outer product matrix of these areas.\n",
    "    \n",
    "    :param feature_class: Path to the input feature class.\n",
    "    :return: A tuple containing a list of areas and the outer product matrix.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract Areas\n",
    "    areas = []\n",
    "    with arcpy.da.SearchCursor(feature_class, [\"SHAPE@AREA\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            areas.append(row[0])  # Append the area of the feature to the list\n",
    "\n",
    "    # Step 2: Create Outer Matrix\n",
    "    areas_array = np.array(areas)  # Convert the list of areas to a NumPy array\n",
    "    outer_matrix = np.outer(areas_array, areas_array)  # Calculate the outer product matrix\n",
    "\n",
    "    return areas, outer_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def CalcEigenmeasures(LandscapeMatrix):\n",
    "    # Define the function to calculate contribution\n",
    "    def calculate_contribution(Lambda, Mij):\n",
    "        num_patches = Mij.shape[0]\n",
    "        contributions = np.zeros(num_patches)\n",
    "        \n",
    "        for ii in range(num_patches):\n",
    "            # Exclude the ii-th row and column from Mij\n",
    "            Mij_sub = np.delete(Mij, ii, axis=0)\n",
    "            Mij_sub = np.delete(Mij_sub, ii, axis=1)\n",
    "            \n",
    "            # Calculate the largest eigenvalue of the modified Mij\n",
    "            eigenvalues = np.linalg.eigvals(Mij_sub)\n",
    "            largest_eigenvalue = np.max(eigenvalues)\n",
    "            \n",
    "            # Calculate the contribution\n",
    "            contributions[ii] = (Lambda - largest_eigenvalue) / Lambda\n",
    "        \n",
    "        return contributions\n",
    "\n",
    "    eigenvalues, eigenvectors = scipy.linalg.eig(Mij)\n",
    "    Le = np.argmax(eigenvalues)\n",
    "    Lambda = np.real(eigenvalues[Le])\n",
    "    \n",
    "    # Approximate stable age distribution = right eigenvector\n",
    "    w0 = np.real(eigenvectors[:, Le])\n",
    "    w = np.abs(w0)\n",
    "    # Reproductive value = left eigenvector\n",
    "    # This equals the right eigenvector of the landscape matrix transposed\n",
    "    V = np.linalg.inv(eigenvectors).conj()\n",
    "    v = np.abs(np.real(V[Le, ]))\n",
    "    \n",
    "    # Contribution of the patch to lambda\n",
    "    # When considering small perturbations, loses a small part of the habitats and habitat degradation\n",
    "    pc = v.T * w# # These are normalized\n",
    "    #pc_small = pc / np.sum(pc)  # Normalize the contributions\n",
    "\n",
    "    # 'pc_s' now contains the normalized contribution of each patch to lambda\n",
    "    # Call the function with your Lambda and Mij data\n",
    "    contributions = calculate_contribution(Lambda, Mij)\n",
    "\n",
    "    # Collect all the measures\n",
    "    EigenMeasures = pd.DataFrame({\"NiNID\":ids,\n",
    "                                  \"PC_small\":pc,\n",
    "                                  \"PC_large\":contributions,\n",
    "                                  \"REv\":w, \n",
    "                                  \"LEv\":v}, index = names)\n",
    "    return EigenMeasures\n",
    "\n",
    "\n",
    "def communities(landscape_matrix):\n",
    "    # Convert the input matrix to a NumPy array\n",
    "    M = np.array(landscape_matrix)\n",
    "    # Calculate the total number of edges/links in the landscape\n",
    "    m = np.sum(M > 0)\n",
    "    \n",
    "    # Calculate the degrees of the vertices/nodes/patches\n",
    "    kj = np.sum(M > 0, axis=0)\n",
    "    ki = np.sum(M > 0, axis=1)\n",
    "    \n",
    "    # Define the modularity matrix\n",
    "    B = M - np.outer(ki, kj) / (2 * m)\n",
    "    # Compute the eigenvalues and eigenvectors of the modularity matrix\n",
    "    ev = np.linalg.eig(B)\n",
    "    \n",
    "    # Find the index of the largest eigenvalue\n",
    "    lmax = np.argmax(np.real(ev[0]))\n",
    "    \n",
    "    # Get the eigenvector corresponding to the largest eigenvalue\n",
    "    W = ev[1][:, lmax]\n",
    "    \n",
    "    # Create a frame with indices and group labels based on the sign of the eigenvector\n",
    "    w_frame = np.column_stack((np.arange(len(W)), np.array([chr(97 + int(x)) for x in np.sign(W) + 1])))\n",
    "    # Generate positions for the nodes for visualization\n",
    "    #pos = nx.spring_layout(nx.from_numpy_matrix(M))\n",
    "    \n",
    "    # Plot the nodes with colors based on their group labels\n",
    "    #plt.scatter(*zip(*pos.values()), c=[ord(x) for x in w_frame[:, 1]])\n",
    "    #plt.show()\n",
    "    print(\"Run loop\")\n",
    "    t = 0\n",
    "    while len(np.unique(w_frame[:, t])) != len(np.unique(w_frame[:, t + 1])):\n",
    "        print(t)\n",
    "        new_col = np.empty(len(w_frame), dtype=object)\n",
    "        \n",
    "        for i in np.unique(w_frame[:, t + 1]):\n",
    "            idx = np.where(w_frame[:, t + 1] == i)[0]\n",
    "            subB = B[np.ix_(idx, idx)]\n",
    "            ev = np.linalg.eig(subB)\n",
    "            lmax = np.argmax(np.real(ev[0]))\n",
    "            W = ev[1][:, lmax]\n",
    "            new_col[idx] = [f\"{w_frame[idx[0], t + 1]}-{chr(97 + int(x))}\" for x in np.sign(W) + 1]\n",
    "        #print(new_col)\n",
    "        w_frame = np.column_stack((w_frame, new_col))\n",
    "        #w_frame[:, t + 1] = np.array([ord(x) for x in w_frame[:, t + 1]])\n",
    "        # Convert the first column of characters to numbers starting from 1\n",
    "        unique_chars, indices = np.unique(w_frame[:, t + 1], return_inverse=True)\n",
    "        w_frame[:, t + 1] = indices + 1\n",
    "        t += 1\n",
    "    \n",
    "    return w_frame[:, :-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Bruk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geodatabase 'Nettverk.gdb' already exists at C:/Users/endofs/OneDrive - Miljødirektoratet/Kartfiler/Nettverk\\Nettverk.gdb\n",
      "Ferdig\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the function.\n",
    "# Set the path where you want to create the file geodatabase\n",
    "out_folder_path = \"C:/Users/endofs/OneDrive - Miljødirektoratet/Kartfiler/Nettverk\"\n",
    "# Set the name for your new file geodatabase\n",
    "gdb_name = \"Nettverk.gdb\"\n",
    "\n",
    "# File to work with\n",
    "input_fc = \"C:/Users/endofs/OneDrive - Miljødirektoratet/Dokumenter/ArcGIS/Projects/MyProject/MyProject.gdb/GRUK\"\n",
    "\n",
    "\n",
    "# Set the workspace environment to your desired directory\n",
    "arcpy.env.workspace = out_folder_path\n",
    "# Enable overwriting of output\n",
    "arcpy.env.overwriteOutput = True\n",
    "gdb_path = create_or_get_gdb(out_folder_path, gdb_name)\n",
    "\n",
    "Distances = calculate_distance_matrix(input_fc, Alpha = 1/1500, ExportLines = False)  # Call the function and store the result.\n",
    "#print(result[0])\n",
    "#print(result[1])\n",
    "areas = calculate_areas_and_outer_matrix(input_fc)\n",
    "Mij = Distances[1] * areas[1]\n",
    "# Get the number of columns in the ndarray to determine the size\n",
    "array_size = Mij.shape[1]\n",
    "# Generate 'names' and 'formats' based on the array size\n",
    "names = []\n",
    "ids = []\n",
    "with arcpy.da.SearchCursor(input_fc, [\"OID@\",\"NiNID\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        names.append(str(row[0]))\n",
    "        ids.append(str(row[1]))\n",
    "\n",
    "formats = ['f8'] * array_size  # Assuming all fields are of type float ('f8')\n",
    "# Create a compound dtype using the names and formats\n",
    "dtype = np.dtype({'names': names, 'formats': formats})\n",
    "# Convert the ndarray to a structured array\n",
    "Mij_str = np.core.records.fromarrays(Mij.transpose(), dtype=dtype)\n",
    "# Enable overwriting of output\n",
    "arcpy.env.overwriteOutput = True\n",
    "# Use the Table To Geodatabase tool to import the CSV into the file geodatabase\n",
    "table_path = out_folder_path+\"/\"+gdb_name+\"/LandscapeMatrix\"\n",
    "if arcpy.Exists(table_path):\n",
    "    arcpy.Delete_management(table_path)\n",
    "arcpy.da.NumPyArrayToTable(Mij_str, table_path)\n",
    "print(\"Ferdig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[36]:18: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                NiNID      PC_small      PC_large           REv           LEv  \\\n",
      "1    NINFP2110013705  4.997741e-24  1.234119e-15  2.235468e-12  2.235658e-12   \n",
      "2    NINFP2010006340  1.169922e-23 -3.173449e-15  3.420397e-12  3.420427e-12   \n",
      "3    NINFP2110029289  1.526052e-24  0.000000e+00  1.235333e-12  1.235337e-12   \n",
      "4    NINFP2010014883  2.101455e-34 -3.349752e-15  3.744973e-17  5.611402e-18   \n",
      "5    NINFP1810016775  1.790253e-35 -1.586725e-15  1.850446e-17  9.674714e-19   \n",
      "..               ...           ...           ...           ...           ...   \n",
      "456  NINFP2110017405  1.091950e-24  5.289082e-16  1.044940e-12  1.044988e-12   \n",
      "457  NINFP2010047611  2.764291e-25 -8.815136e-16  5.257722e-13  5.257584e-13   \n",
      "458  NINFP2110069698  2.764357e-25 -8.815136e-16  5.257722e-13  5.257709e-13   \n",
      "459  NINFP2010047823  5.064657e-24 -2.468238e-15  2.250495e-12  2.250464e-12   \n",
      "460  NINFP2110062376  5.064651e-24 -2.468238e-15  2.250495e-12  2.250461e-12   \n",
      "\n",
      "            Area  \n",
      "1    1063.374975  \n",
      "2     619.418522  \n",
      "3     565.190899  \n",
      "4     400.369786  \n",
      "5     257.800531  \n",
      "..           ...  \n",
      "456   830.332203  \n",
      "457   190.272960  \n",
      "458   190.272960  \n",
      "459   483.941095  \n",
      "460   483.941095  \n",
      "\n",
      "[460 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "EigenMeasures = CalcEigenmeasures(Mij)\n",
    "EigenMeasures[\"Area\"] = areas[0]\n",
    "print(EigenMeasures.head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run loop\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[36]:93: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Ferdig\n"
     ]
    }
   ],
   "source": [
    "Coms = communities(Mij)\n",
    "print(\"Ferdig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped by 'Group':\n",
      "<bound method NDFrame.head of                          NiNID      PC_small      PC_large           REv  \\\n",
      "Community                                                                  \n",
      "1         0    NINFP2110013705  4.997741e-24  1.234119e-15  2.235468e-12   \n",
      "          2    NINFP2110029289  1.526052e-24  0.000000e+00  1.235333e-12   \n",
      "          7    NINFP2110061632  6.310550e-21 -1.234119e-15  7.943885e-11   \n",
      "          11   NINFP2110061968  2.181544e-24 -3.878660e-15  1.477015e-12   \n",
      "          13   NINFP2110017878  7.139731e-25 -2.644541e-15  8.450128e-13   \n",
      "...                        ...           ...           ...           ...   \n",
      "19        491  NINFP1910017345  1.226786e-02  1.240306e-02  1.107604e-01   \n",
      "          492  NINFP1910017345  1.226786e-02  1.240306e-02  1.107604e-01   \n",
      "          493  NINFP1910018441  3.473221e-11  3.472441e-11  5.893404e-06   \n",
      "          497  NINFP1910018413  8.198483e-03  8.029367e-03  9.054547e-02   \n",
      "          501  NINFP2110059634  1.226786e-02  1.240306e-02  1.107604e-01   \n",
      "\n",
      "                        LEv         Area Community  PC_large_Area  \\\n",
      "Community                                                           \n",
      "1         0    2.235658e-12  1063.374975         1   1.160568e-18   \n",
      "          2    1.235337e-12   565.190899         1   0.000000e+00   \n",
      "          7    7.943908e-11  4422.831211         1  -2.790337e-19   \n",
      "          11   1.476995e-12   626.697639         1  -6.189045e-18   \n",
      "          13   8.449258e-13   338.157147         1  -7.820450e-18   \n",
      "...                     ...          ...       ...            ...   \n",
      "19        491  1.107604e-01  1690.997825        19   7.334756e-06   \n",
      "          492  1.107604e-01  1690.997825        19   7.334756e-06   \n",
      "          493  5.893404e-06   799.204875        19   4.344870e-14   \n",
      "          497  9.054547e-02  3807.376620        19   2.108898e-06   \n",
      "          501  1.107604e-01  1690.997825        19   7.334756e-06   \n",
      "\n",
      "              PC_large_klasse PC_small_klasse PC_large_klasse_None  \\\n",
      "Community                                                            \n",
      "1         0               Høy         Middels                  Høy   \n",
      "          2               Høy         Middels                  Høy   \n",
      "          7           Middels             Høy              Middels   \n",
      "          11              Lav         Middels                  Lav   \n",
      "          13              Lav         Middels                  Lav   \n",
      "...                       ...             ...                  ...   \n",
      "19        491             Høy             Høy                  Høy   \n",
      "          492             Høy             Høy                  Høy   \n",
      "          493             Høy             Høy                  Høy   \n",
      "          497             Høy             Høy                  Høy   \n",
      "          501             Høy             Høy                  Høy   \n",
      "\n",
      "              PC_large_klasse_Community  \n",
      "Community                                \n",
      "1         0                         Høy  \n",
      "          2                         Høy  \n",
      "          7                     Middels  \n",
      "          11                        Lav  \n",
      "          13                        Lav  \n",
      "...                                 ...  \n",
      "19        491                       Høy  \n",
      "          492                       Høy  \n",
      "          493                       Høy  \n",
      "          497                       Høy  \n",
      "          501                       Høy  \n",
      "\n",
      "[502 rows x 12 columns]>\n",
      "<bound method NDFrame.head of                NiNID      PC_small      PC_large           REv           LEv  \\\n",
      "0    NINFP2110013705  4.997741e-24  1.234119e-15  2.235468e-12  2.235658e-12   \n",
      "1    NINFP2010006340  1.169922e-23 -3.173449e-15  3.420397e-12  3.420427e-12   \n",
      "2    NINFP2110029289  1.526052e-24  0.000000e+00  1.235333e-12  1.235337e-12   \n",
      "3    NINFP2010014883  2.101455e-34 -3.349752e-15  3.744973e-17  5.611402e-18   \n",
      "4    NINFP1810016775  1.790253e-35 -1.586725e-15  1.850446e-17  9.674714e-19   \n",
      "..               ...           ...           ...           ...           ...   \n",
      "497  NINFP1910018413  8.198483e-03  8.029367e-03  9.054547e-02  9.054547e-02   \n",
      "498  NINFP1910010840  8.391615e-22 -2.997146e-15  2.896829e-11  2.896827e-11   \n",
      "499  NINFP2110012433  2.979365e-26 -5.289082e-15  1.725889e-13  1.726279e-13   \n",
      "500                   1.611879e-40 -1.763027e-15  4.584858e-23  3.515657e-18   \n",
      "501  NINFP2110059634  1.226786e-02  1.240306e-02  1.107604e-01  1.107604e-01   \n",
      "\n",
      "            Area Community  PC_large_Area PC_large_klasse PC_small_klasse  \\\n",
      "0    1063.374975         1   1.160568e-18             Høy         Middels   \n",
      "1     619.418522         2  -5.123271e-18             Lav         Middels   \n",
      "2     565.190899         1   0.000000e+00             Høy         Middels   \n",
      "3     400.369786         4  -8.366645e-18             Lav             Lav   \n",
      "4     257.800531         5  -6.154854e-18         Middels             Lav   \n",
      "..           ...       ...            ...             ...             ...   \n",
      "497  3807.376620        19   2.108898e-06             Høy             Høy   \n",
      "498  1712.965816         1  -1.749683e-18             Lav             Høy   \n",
      "499   191.915653         1  -2.755941e-17             Lav             Lav   \n",
      "500   452.309400         3  -3.897835e-18         Middels             Lav   \n",
      "501  1690.997825        19   7.334756e-06             Høy             Høy   \n",
      "\n",
      "    PC_large_klasse_None  \n",
      "0                    Høy  \n",
      "1                    Lav  \n",
      "2                    Høy  \n",
      "3                    Lav  \n",
      "4                Middels  \n",
      "..                   ...  \n",
      "497                  Høy  \n",
      "498                  Lav  \n",
      "499                  Lav  \n",
      "500              Middels  \n",
      "501                  Høy  \n",
      "\n",
      "[502 rows x 11 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Function to bin scores within each group or for the entire DataFrame\n",
    "def bin_scores(df, column='PC_large', group=None, cuts=None, labels=None):\n",
    "    if cuts is None:\n",
    "        cuts = np.quantile(df[column], [1/3, 2/3])\n",
    "    if labels is None:\n",
    "        labels = ['Lav', 'Middels', 'Høy']\n",
    "    \n",
    "    def bin_group(group_df):\n",
    "        group_cuts = np.quantile(group_df[column], [1/3, 2/3]) if cuts is None else cuts\n",
    "        group_df[f'{column}_klasse_{group}'] = pd.cut(group_df[column], bins=[-np.inf, *group_cuts, np.inf], labels=labels)\n",
    "        return group_df\n",
    "    \n",
    "    if group:\n",
    "        return df.groupby(group).apply(bin_group)\n",
    "    else:\n",
    "        return bin_group(df)\n",
    "\n",
    "# Apply the function to the DataFrame with and without grouping\n",
    "EigenAnalyses_grouped = bin_scores(EigenAnalyses, group='Community')\n",
    "EigenAnalyses_grouped2 = bin_scores(EigenAnalyses)\n",
    "\n",
    "print(\"Grouped by 'Group':\")\n",
    "print(EigenAnalyses_grouped.head)\n",
    "print(EigenAnalyses_grouped2.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                NiNID      PC_small      PC_large           REv           LEv  \\\n",
      "0    NINFP2110013705  4.997741e-24  1.234119e-15  2.235468e-12  2.235658e-12   \n",
      "1    NINFP2010006340  1.169922e-23 -3.173449e-15  3.420397e-12  3.420427e-12   \n",
      "2    NINFP2110029289  1.526052e-24  0.000000e+00  1.235333e-12  1.235337e-12   \n",
      "3    NINFP2010014883  2.101455e-34 -3.349752e-15  3.744973e-17  5.611402e-18   \n",
      "4    NINFP1810016775  1.790253e-35 -1.586725e-15  1.850446e-17  9.674714e-19   \n",
      "..               ...           ...           ...           ...           ...   \n",
      "497  NINFP1910018413  8.198483e-03  8.029367e-03  9.054547e-02  9.054547e-02   \n",
      "498  NINFP1910010840  8.391615e-22 -2.997146e-15  2.896829e-11  2.896827e-11   \n",
      "499  NINFP2110012433  2.979365e-26 -5.289082e-15  1.725889e-13  1.726279e-13   \n",
      "500                   1.611879e-40 -1.763027e-15  4.584858e-23  3.515657e-18   \n",
      "501  NINFP2110059634  1.226786e-02  1.240306e-02  1.107604e-01  1.107604e-01   \n",
      "\n",
      "            Area Community  PC_large_Area PC_large_klasse PC_small_klasse  \n",
      "0    1063.374975         1   1.160568e-18             Høy         Middels  \n",
      "1     619.418522         2  -5.123271e-18             Lav         Middels  \n",
      "2     565.190899         1   0.000000e+00             Høy         Middels  \n",
      "3     400.369786         4  -8.366645e-18             Lav             Lav  \n",
      "4     257.800531         5  -6.154854e-18         Middels             Lav  \n",
      "..           ...       ...            ...             ...             ...  \n",
      "497  3807.376620        19   2.108898e-06             Høy             Høy  \n",
      "498  1712.965816         1  -1.749683e-18             Lav             Høy  \n",
      "499   191.915653         1  -2.755941e-17             Lav             Lav  \n",
      "500   452.309400         3  -3.897835e-18         Middels             Lav  \n",
      "501  1690.997825        19   7.334756e-06             Høy             Høy  \n",
      "\n",
      "[502 rows x 10 columns]>\n"
     ]
    }
   ],
   "source": [
    "Gr = \"NiNID\"\n",
    "idx = Coms.shape[1]\n",
    "Coms2 = pd.DataFrame(np.column_stack((np.array(EigenMeasures)[:,0],\n",
    "                        Coms[:,idx-1])))\n",
    "Coms2.columns = [Gr,\"Community\"]\n",
    "EigenAnalyses = pd.merge(EigenMeasures,Coms2, on = Gr)\n",
    "EigenAnalyses['PC_large_Area'] = EigenAnalyses['PC_large']/EigenAnalyses['Area']\n",
    "# Define thresholds and labels\n",
    "cuts = np.quantile(EigenAnalyses['PC_large'], [1/3, 2/3])\n",
    "cuts2 = np.quantile(EigenAnalyses['PC_small'], [1/3, 2/3])\n",
    "\n",
    "labels = ['Lav', 'Middels', 'Høy']\n",
    "# Bin the scores and write the result to a new column\n",
    "EigenAnalyses['PC_large_klasse'] = pd.cut(EigenAnalyses['PC_large'], bins=[-np.inf, *cuts, np.inf], labels=labels)\n",
    "#EigenAnalyses['PC_small_klasse'] = pd.cut(EigenAnalyses['PC_small'], bins=[-np.inf, *cuts2, np.inf], labels=labels)\n",
    "\n",
    "print(EigenAnalyses.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results back to file geodatabase etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beregne sensitiviteter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                NiNID          NiNID_2   Sensitivity Sensitivity_klasse  \\\n",
      "5     NINFP1910019514  NINFP2110013705  2.256702e-13                Høy   \n",
      "10    NINFP1910019576  NINFP2110013705  9.281158e-14                Høy   \n",
      "14    NINFP1910019570  NINFP2110013705  3.164192e-13                Høy   \n",
      "33    NINFP1910017755  NINFP2110013705  3.915374e-14                Høy   \n",
      "35    NINFP1910019478  NINFP2110013705  2.809535e-13                Høy   \n",
      "...               ...              ...           ...                ...   \n",
      "2687  NINFP1910019478  NINFP2010048283  6.124512e-13                Høy   \n",
      "2690  NINFP2010006408  NINFP2010048283  4.262892e-21                Høy   \n",
      "2695  NINFP2010058626  NINFP2010048283  1.094992e-19                Høy   \n",
      "2697  NINFP1910019588  NINFP2010048283  3.351486e-13                Høy   \n",
      "2701  NINFP1910019566  NINFP2010048283  4.349292e-13                Høy   \n",
      "\n",
      "     Community_1 Community_2  \n",
      "5             19           1  \n",
      "10            19           1  \n",
      "14            19           1  \n",
      "33            19           1  \n",
      "35            19           1  \n",
      "...          ...         ...  \n",
      "2687          19           1  \n",
      "2690           2           1  \n",
      "2695          19           1  \n",
      "2697          19           1  \n",
      "2701          19           1  \n",
      "\n",
      "[760 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "sensMatrix = np.outer(EigenAnalyses[\"REv\"][0:40],EigenAnalyses[\"REv\"][0:40])\n",
    "sensDF = pd.DataFrame(sensMatrix, index = EigenAnalyses[\"NiNID\"][0:40], columns = EigenAnalyses[\"NiNID\"][0:40])\n",
    "sensDF = pd.melt(sensDF.reset_index(), id_vars=['NiNID'], value_vars=EigenAnalyses[\"NiNID\"][0:40], \n",
    "                    var_name='NiNID_2', value_name='Sensitivity')\n",
    "\n",
    "# Define thresholds and labels\n",
    "cuts = np.quantile(sensDF['Sensitivity'], [1/3, 2/3])\n",
    "labels = ['Lav', 'Middels', 'Høy']\n",
    "\n",
    "# Bin the scores and write the result to a new column\n",
    "sensDF['Sensitivity_klasse'] = pd.cut(sensDF['Sensitivity'], bins=[-np.inf, *cuts, np.inf], labels=labels)\n",
    "\n",
    "tmpTBL = EigenAnalyses[['NiNID', \"Community\"]]\n",
    "# Merge Group from Table 1 into Table 2 based on ID1\n",
    "sensDF2 = sensDF.merge(tmpTBL, left_on='NiNID', right_on='NiNID', how='left').rename(columns={'Community': 'Community_1'})#.drop(columns=['NiNID'])\n",
    "#print(sensDF2.merge(tmpTBL, left_on='NiNID_2', right_on='NiNID', how='left'))\n",
    "# Merge Group from Table 1 into Table 2 based on ID2\n",
    "sensDF2 = sensDF2.merge(tmpTBL, left_on='NiNID_2', right_on='NiNID', how='left').rename(columns={'Community': 'Community_2','NiNID_x': 'NiNID'}).drop(columns=['NiNID_y'])\n",
    "\n",
    "subSens = sensDF2[sensDF2['Sensitivity_klasse']=='Høy']\n",
    "print(subSens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEGG TIL EIGEN-MÅLENE TIL GEOMETRIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PC_small', 'PC_large', 'REv', 'LEv', 'Area', 'Community', 'PC_large_Area', 'PC_large_klasse', 'PC_small_klasse']\n",
      "[dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('O'), dtype('float64'), CategoricalDtype(categories=['Lav', 'Middels', 'Høy'], ordered=True), CategoricalDtype(categories=['Lav', 'Middels', 'Høy'], ordered=True)]\n",
      "<bound method Series.items of NiNID                object\n",
      "PC_small            float64\n",
      "PC_large            float64\n",
      "REv                 float64\n",
      "LEv                 float64\n",
      "Area                float64\n",
      "Community            object\n",
      "PC_large_Area       float64\n",
      "PC_large_klasse    category\n",
      "PC_small_klasse    category\n",
      "dtype: object>\n"
     ]
    }
   ],
   "source": [
    "print(list(EigenAnalyses.columns)[1:])\n",
    "print(list(EigenAnalyses.dtypes[1:]))\n",
    "print(EigenAnalyses.dtypes.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Write the measure to the input_fc\n",
    "# Add fields for \n",
    "# Add a new field to the feature class for each column in the DataFrame\n",
    "#fields_to_add = [\"PC_small\", \"PC_large\", \"REv\", \"LEv\",\"Community\"]\n",
    "# Read all columns except the first one, which is NiNID\n",
    "fields_to_add = list(EigenAnalyses.columns)[1:]\n",
    "\n",
    "# Map pandas dtypes to ArcPy field types\n",
    "dtype_mapping = {'int64': 'LONG', 'float64': 'DOUBLE', 'object': 'TEXT','category': 'TEXT'}\n",
    "\n",
    "# Add fields to the feature class based on DataFrame column data types\n",
    "for field, dtype in EigenAnalyses.dtypes.items():\n",
    "    if field != 'ID':  # Skip the ID column\n",
    "        arcpy.management.AddField(input_fc, field, dtype_mapping[str(dtype)])\n",
    "\n",
    "\n",
    "# Define the fields to use in the cursor, including the OID field\n",
    "fields = ['NiNID'] + fields_to_add\n",
    "\n",
    "# Use an update cursor to iterate through the feature class\n",
    "with arcpy.da.UpdateCursor(input_fc, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        oid_value = row[0]  # Get the OID value from the feature class\n",
    "        # Find the corresponding row in the DataFrame based on the OID value\n",
    "        df_row = EigenAnalyses.loc[EigenAnalyses['NiNID'] == oid_value]\n",
    "        if not df_row.empty:\n",
    "            # Update the fields in the feature class with values from the DataFrame\n",
    "            for i, field in enumerate(fields_to_add):\n",
    "                row[i + 1] = df_row[field].values[0]  # Skip the first field (OID)\n",
    "            cursor.updateRow(row)\n",
    "        else:\n",
    "            print(f\"No matching row found for OID {oid_value}\")            \n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    }
   ],
   "source": [
    "print(len(areas[0]))\n",
    "#plt.hist(EigenAnalyses['Area'])\n",
    "#plt.scatter(1,areas[0])\n",
    "#plt.scatter(EigenAnalyses['PC_large']/EigenAnalyses['Area'],EigenAnalyses['PC_small'])\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lag koblingene som egen fil med sensitivetsmål\n",
    "\n",
    "Neste steg vil være at en tar utgangspunkt i sensitivitetsmatrisen. Henter ut f.eks. de mest verdifulle lenkene, og lager en linjegeometri basert på denne. Det vil da være av interesse å skille på lenker som går mellom og innad klynger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: lørdag 24. august 2024 22:22:40\",\"Succeeded at lørdag 24. august 2024 22:22:42 (Elapsed Time: 1,87 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\endofs\\\\OneDrive - Miljødirektoratet\\\\Kartfiler\\\\Nettverk\\\\Nettverk.gdb\\\\Pts'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the input to points, for better visualization\n",
    "arcpy.FeatureToPoint_management(input_fc, out_folder_path+\"/\"+gdb_name+\"/Pts\"\n",
    ", \"INSIDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the points among which the sensitivities are highest, as lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "C:/Users/endofs/OneDrive - Miljødirektoratet/Kartfiler/Nettverk//Nettverk.gdb\n",
      "Query\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: lørdag 24. august 2024 22:22:42\",\"Succeeded at lørdag 24. august 2024 22:22:42 (Elapsed Time: 0,20 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:/Users/endofs/OneDrive - Miljødirektoratet/Kartfiler/Nettverk/Nettverk.gdb/subset_layer'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of IDs to subset\n",
    "print(\"H\")\n",
    "# Define the output feature class\n",
    "output_fc = out_folder_path+\"//\"+gdb_name\n",
    "print(output_fc)\n",
    "id_list = list(subSens[\"NiNID\"])+list(subSens[\"NiNID\"])\n",
    "\n",
    "# Create a SQL query to select the IDs\n",
    "id_string = ', '.join(map(str, id_list))\n",
    "sql_query = '\"NiNID\" in ({:s})'.format(','.join([\"'\" + str(x) +\"'\" for x in id_list]))\n",
    "# Create a feature layer with the subset of points\n",
    "print(\"Query\")\n",
    "subset_layer = \"subset_layer\"\n",
    "#arcpy.MakeFeatureLayer_management(input_fc, subset_layer, )\n",
    "\n",
    "arcpy.MakeFeatureLayer_management(input_fc, out_folder_path+\"/\"+gdb_name+\"/subset_layer\", sql_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOrsøk på kode hvor vi legger til linjer baser på hvorvidt de er i subsens df eller ikke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[56]:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dyad field updated, rows deleted, and lines created successfully!\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "\n",
    "# Set the workspace (update this path to your actual workspace)\n",
    "arcpy.env.workspace = out_folder_path + \"/\" + gdb_name\n",
    "\n",
    "# Define the input feature class\n",
    "tmp_fc = out_folder_path + \"/\" + gdb_name + \"/subset_layer\"\n",
    "\n",
    "# Sample DataFrame (replace with your actual DataFrame)\n",
    "subSens.loc[:, 'Dyad'] = subSens.apply(lambda row: f\"{min(row['NiNID'], row['NiNID_2'])}_{max(row['NiNID'], row['NiNID_2'])}\", axis=1)\n",
    "\n",
    "# Get the set of Dyads from the DataFrame\n",
    "df_dyads = set(subSens['Dyad'])\n",
    "\n",
    "# Create an output feature class for the lines\n",
    "tmp_output_fc = \"Linker\"\n",
    "arcpy.CreateFeatureclass_management(arcpy.env.workspace, tmp_output_fc, \"POLYLINE\")\n",
    "\n",
    "# Add fields to the output feature class\n",
    "arcpy.AddField_management(tmp_output_fc, \"NiNID_1\", \"TEXT\")\n",
    "arcpy.AddField_management(tmp_output_fc, \"NiNID_2\", \"TEXT\")\n",
    "arcpy.AddField_management(tmp_output_fc, \"Dyad\", \"TEXT\")\n",
    "\n",
    "# Use a search cursor to iterate through the polygons\n",
    "polygons = []\n",
    "with arcpy.da.SearchCursor(tmp_fc, [\"SHAPE@\", \"NiNID\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        polygons.append((row[0], row[1]))\n",
    "\n",
    "# Use an insert cursor to create lines between polygons\n",
    "with arcpy.da.InsertCursor(tmp_output_fc, [\"SHAPE@\", \"NiNID_1\", \"NiNID_2\", \"Dyad\"]) as cursor:\n",
    "    for i in range(len(polygons)):\n",
    "        for j in range(i + 1, len(polygons)):\n",
    "            start_polygon, start_ninid = polygons[i]\n",
    "            end_polygon, end_ninid = polygons[j]\n",
    "            start_point = start_polygon.centroid\n",
    "            end_point = end_polygon.centroid\n",
    "            dyad = f\"{min(start_ninid, end_ninid)}_{max(start_ninid, end_ninid)}\"\n",
    "            if dyad in df_dyads:\n",
    "                line = arcpy.Polyline(arcpy.Array([start_point, end_point]))\n",
    "                cursor.insertRow([line, start_ninid, end_ninid, dyad])\n",
    "\n",
    "print(\"Dyad field updated, rows deleted, and lines created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD CODE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
